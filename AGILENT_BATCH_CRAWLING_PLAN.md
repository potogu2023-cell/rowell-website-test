# Agilent品牌批量爬取计划

**日期**: 2025-11-05  
**制定人**: ROWELL网站总工程师  
**状态**: ✅ 已批准，准备执行

---

## 📊 项目概述

### 目标
完成Agilent品牌全部630个产品的数据采集和导入，建立高质量的产品数据库。

### 背景
- ✅ 已完成示例数据验证（4个产品，质量优秀）
- ✅ 已验证爬虫程序可用性（29个文件，程序质量A级）
- ✅ 已测试数据导入流程（3/4成功，流程验证通过）
- ✅ 技术可行性已确认，可以进入批量爬取阶段

---

## 🎯 爬取范围

### 品牌信息
- **品牌名称**: Agilent Technologies
- **产品总数**: 630个
- **数据来源**: `/home/ubuntu/rowell-website-test/product_list_for_crawler_2025-11-05.csv`

### 产品类型分布

| 产品类型 | 数量 | 占比 | 优先级 |
|---------|------|------|--------|
| Columns（色谱柱） | 208个 | 33.0% | P1 |
| Other Supplies（其他耗材） | 204个 | 32.4% | P2 |
| Vials & Caps（样品瓶和盖） | 185个 | 29.4% | P2 |
| Fittings & Tubing（接头和管路） | 33个 | 5.2% | P3 |
| **总计** | **630个** | **100%** | - |

---

## 📋 执行策略

### 阶段1：小批量测试（50-100个产品）

**目的**: 验证批量爬取的稳定性和数据质量

**范围**: 
- 从各产品类型中抽取50-100个产品
- 覆盖所有主要产品类型
- 包含不同价格区间和规格

**预期时间**: 3-5分钟

**验收标准**:
- 成功率 ≥ 95%
- 产品名称完整性 = 100%
- 规格字段数 ≥ 3个
- 描述覆盖率 ≥ 70%

**决策点**:
- ✅ 如果测试通过 → 进入阶段2
- ⚠️ 如果发现问题 → 调整策略，重新测试

### 阶段2：全量爬取（630个产品）

**目的**: 完成Agilent品牌所有产品的数据采集

**范围**: 全部630个产品

**预期时间**: 45-60分钟（基于爬虫团队估算）

**执行方式**:
1. 使用已验证的爬虫程序
2. 按产品类型分批执行（可选）
3. 实时监控爬取进度和错误
4. 生成详细的爬取报告

**验收标准**:
- 成功率 ≥ 90%
- 产品名称完整性 = 100%
- 规格完整性 ≥ 90%（≥3个字段）
- 描述覆盖率 ≥ 70%
- 描述质量分布：A/B级 ≥ 30%

---

## 📊 数据质量标准

### 必需字段（100%完整性）

| 字段 | 要求 | 验证规则 |
|------|------|---------|
| productId | 必需 | 格式：AGIL-XXXXXX |
| partNumber | 必需 | 原厂零件号 |
| brand | 必需 | "Agilent" |
| name | 必需 | 完整产品名称，无截断 |

### 重要字段（≥90%完整性）

| 字段 | 要求 | 验证规则 |
|------|------|---------|
| specifications | 重要 | 有效JSON，≥3个字段 |
| description | 重要 | 非空，≥20字符 |

### 可选字段（尽力而为）

| 字段 | 要求 | 说明 |
|------|------|------|
| imageUrl | 可选 | 产品图片URL |
| catalogUrl | 可选 | 产品目录页URL |
| technicalDocUrl | 可选 | 技术文档URL |
| detailedDescription | 可选 | 详细描述 |

### 描述质量分级

| 等级 | 长度范围 | 质量评价 | 目标占比 |
|------|---------|---------|---------|
| A级 (high) | ≥100字符 | 优秀 | ≥20% |
| B级 (medium) | 50-99字符 | 良好 | ≥10% |
| C级 (low) | 20-49字符 | 合格 | ≥30% |
| D级 (extracted) | <20字符 | 提取 | ≤30% |
| N/A (none) | 0字符 | 缺失 | ≤10% |

**综合目标**: A/B级描述占比 ≥ 30%

---

## 🛠️ 技术方案

### 爬虫程序

**使用工具**: 爬虫团队交付的Python程序包
- 文件: `hplc_crawler_delivery.tar.gz`
- 主程序: `agilent_crawler.py`
- 辅助工具: `batch_crawl_helper.py`

**执行命令**（示例）:
```bash
# 小批量测试（50个产品）
python batch_crawl_helper.py --brand agilent --limit 50 --output test_results.csv

# 全量爬取（630个产品）
python batch_crawl_helper.py --brand agilent --output agilent_full_results.csv
```

### 数据导入

**导入脚本**: `/home/ubuntu/rowell-website-test/import-crawler-data.mjs`

**执行流程**:
1. 读取爬取结果CSV文件
2. 验证数据格式和完整性
3. 匹配数据库中的产品（基于productId）
4. 更新产品信息（name, description, specifications等）
5. 生成导入报告

**冲突解决策略**:
- 如果产品存在：更新字段（name, description, specifications, descriptionQuality）
- 如果产品不存在：跳过（记录到日志）
- 保留原有字段：brand, partNumber, prefix, productId

### 质量验证

**验证脚本**: 自动生成验证报告

**验证内容**:
1. 数据完整性检查
2. 字段格式验证
3. 质量指标统计
4. 问题识别和分类

---

## 📅 时间计划

### 阶段1：小批量测试

| 任务 | 负责方 | 预计时间 | 交付物 |
|------|--------|---------|--------|
| 执行爬取（50-100个产品） | 爬虫团队 | 3-5分钟 | test_results.csv |
| 数据验证 | ROWELL团队 | 5-10分钟 | 验证报告 |
| 决策评审 | ROWELL团队 | 5分钟 | 批准/调整 |
| **阶段1总计** | - | **15-25分钟** | - |

### 阶段2：全量爬取

| 任务 | 负责方 | 预计时间 | 交付物 |
|------|--------|---------|--------|
| 执行爬取（630个产品） | 爬虫团队 | 45-60分钟 | agilent_full_results.csv |
| 数据导入 | ROWELL团队 | 10-15分钟 | 导入报告 |
| 质量验证 | ROWELL团队 | 15-20分钟 | 质量报告 |
| 问题修复（如需要） | 爬虫团队 | 0-30分钟 | 修复数据 |
| **阶段2总计** | - | **70-125分钟** | - |

### 总体时间

**最快**: 1.5小时（无问题情况）  
**正常**: 2-3小时（含小问题修复）  
**最慢**: 4-5小时（含较多问题修复）

---

## ✅ 验收标准

### 数据质量验收

| 指标 | 目标 | 验收标准 |
|------|------|---------|
| 成功率 | ≥90% | 至少567个产品成功爬取 |
| 产品名称完整性 | 100% | 所有产品名称完整无截断 |
| 规格完整性 | ≥90% | 至少567个产品有≥3个规格字段 |
| 描述覆盖率 | ≥70% | 至少441个产品有描述 |
| A/B级描述占比 | ≥30% | 至少189个产品有高质量描述 |

### 程序质量验收

| 指标 | 要求 | 验收标准 |
|------|------|---------|
| 错误处理 | 完善 | 所有错误都有日志记录 |
| 进度显示 | 清晰 | 实时显示爬取进度 |
| 报告生成 | 详细 | 包含统计数据和问题列表 |
| 可重复性 | 高 | 可以重新爬取失败的产品 |

### 交付物验收

| 交付物 | 格式 | 内容要求 |
|--------|------|---------|
| 爬取结果CSV | UTF-8编码 | 包含所有必需字段 |
| 爬取报告 | Markdown/PDF | 详细的统计和分析 |
| 错误日志 | 文本文件 | 所有错误和警告 |
| 质量报告 | Markdown/PDF | 数据质量评估 |

---

## 🚨 风险管理

### 潜在风险

| 风险 | 可能性 | 影响 | 应对策略 |
|------|--------|------|---------|
| 网站反爬虫机制 | 中 | 高 | 降低爬取速度，增加延迟 |
| 数据格式变化 | 低 | 中 | 灵活的解析逻辑，人工复核 |
| 网络不稳定 | 中 | 中 | 重试机制，断点续传 |
| 描述缺失普遍 | 中 | 低 | 降级处理，接受提取描述 |
| 规格信息不一致 | 低 | 低 | 标准化处理，人工复核 |

### 应急预案

**场景1：爬取成功率低于80%**
- 暂停爬取
- 分析失败原因
- 调整爬取策略
- 重新测试

**场景2：数据质量不达标**
- 识别问题产品类型
- 针对性优化爬取逻辑
- 重新爬取问题产品
- 人工复核关键数据

**场景3：时间超出预期**
- 评估已完成部分的质量
- 决定是否分批完成
- 调整后续计划

---

## 📊 成功指标

### 短期指标（本次爬取）

- ✅ 630个Agilent产品数据采集完成
- ✅ 数据质量达到验收标准
- ✅ 数据成功导入数据库
- ✅ 网站产品展示正常

### 中期指标（1周内）

- ✅ 完成其他2-3个品牌的爬取
- ✅ 累计完成1,000+产品数据
- ✅ 建立稳定的爬取和导入流程

### 长期指标（2周内）

- ✅ 完成全部11个品牌的爬取
- ✅ 累计完成2,689个产品数据
- ✅ 数据质量全面达标
- ✅ 网站产品库完善

---

## 📞 沟通机制

### 进度汇报

**频率**: 
- 阶段1：完成后立即汇报
- 阶段2：每完成200个产品汇报一次

**内容**:
- 已完成产品数量
- 成功率和质量指标
- 遇到的问题和解决方案
- 预计完成时间

### 问题升级

**一般问题**（成功率80-90%）:
- 爬虫团队自行解决
- 完成后汇报

**重要问题**（成功率70-80%）:
- 立即通知ROWELL团队
- 共同讨论解决方案

**严重问题**（成功率<70%）:
- 立即暂停爬取
- 紧急会议讨论
- 重新评估策略

---

## 📋 检查清单

### 爬取前检查

- [x] 爬虫程序已验证可用
- [x] 产品清单文件已准备（630个产品）
- [x] 数据导入脚本已准备
- [x] 质量验证标准已确定
- [x] 时间计划已制定
- [x] 风险应对策略已准备

### 爬取中监控

- [ ] 实时监控爬取进度
- [ ] 记录错误和警告
- [ ] 检查数据质量
- [ ] 及时处理异常

### 爬取后验证

- [ ] 验证数据完整性
- [ ] 检查数据质量指标
- [ ] 生成质量报告
- [ ] 导入数据库
- [ ] 验证网站展示

---

## 🎯 下一步行动

### 立即执行

1. **向爬虫团队交付任务指令**
   - 提供本计划文档
   - 提供产品清单CSV文件
   - 明确质量标准和时间要求

2. **准备数据接收和验证环境**
   - 确保数据导入脚本可用
   - 准备质量验证工具
   - 准备数据库备份

### 等待交付

3. **接收爬取结果**
   - 阶段1：test_results.csv（50-100个产品）
   - 阶段2：agilent_full_results.csv（630个产品）

4. **执行验证和导入**
   - 运行数据验证
   - 执行数据导入
   - 生成质量报告

### 后续计划

5. **评估结果并规划下一品牌**
   - 总结经验教训
   - 优化流程和标准
   - 启动下一品牌爬取

---

## 📄 附录

### A. 产品清单文件

**文件路径**: `/home/ubuntu/rowell-website-test/product_list_for_crawler_2025-11-05.csv`

**格式**:
```csv
productId,partNumber,brand,name
AGIL-000001,000001,Agilent,Product Name 1
AGIL-000002,000002,Agilent,Product Name 2
...
```

**Agilent产品数**: 630个

### B. 爬虫程序文件

**文件路径**: `/home/ubuntu/upload/hplc_crawler_delivery.tar.gz`

**解压后目录结构**:
```
crawler_project/
├── README.md
├── product_crawler.py
├── agilent_crawler.py
├── batch_crawl_helper.py
├── data/
├── output/
├── docs/
└── logs/
```

### C. 数据导入脚本

**文件路径**: `/home/ubuntu/rowell-website-test/import-crawler-data.mjs`

**使用方法**:
```bash
node import-crawler-data.mjs /path/to/crawler_results.csv
```

### D. 参考文档

1. **爬虫任务指令**: `CRAWLER_TEXT_INFO_INSTRUCTIONS.md`
2. **策略更新文档**: `STRATEGY_UPDATE_SUMMARY.md`
3. **数据质量评估**: `DATA_QUALITY_ASSESSMENT.md`
4. **交付验证报告**: `CRAWLER_DELIVERY_VERIFICATION_REPORT.md`

---

**计划制定日期**: 2025-11-05  
**计划批准人**: ROWELL网站总工程师  
**计划状态**: ✅ 已批准，准备执行

---

**下一步**: 生成爬虫任务指令文档，交付给爬虫团队执行 🚀
