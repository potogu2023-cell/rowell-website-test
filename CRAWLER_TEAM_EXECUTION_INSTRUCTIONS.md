# 爬虫团队执行指令 - Agilent全量爬取

**任务编号**: ROWELL-CRAWLER-AGILENT-FULL-001  
**发布时间**: 2025-11-05  
**优先级**: 🔴 高优先级  
**状态**: ✅ 测试验证通过，准备执行全量爬取

---

## ✅ 测试验证结果

我们已经完成了测试数据的导入验证：

**测试数据（10个产品）**：
- ✅ 成功导入：9/10个产品
- ✅ 数据质量：优秀（6个A级，3个B级）
- ✅ 规格完整性：平均11.4个字段
- ✅ 数据库更新：正常
- ✅ 导入流程：验证通过

**结论**：数据格式和质量完全符合要求，可以进行全量爬取！

---

## 🎯 执行任务

### 任务目标
完成630个Agilent产品的全量数据爬取

### 执行方式
使用已交付的爬虫程序包中的一键执行脚本

### 执行步骤

#### 1. 进入爬虫项目目录
```bash
cd /path/to/crawler_project
```

#### 2. 确认环境准备
```bash
# 检查Python版本
python3.11 --version

# 检查依赖
python3.11 -c "import selenium; print('Selenium OK')"

# 检查Chrome
google-chrome --version
```

#### 3. 执行一键脚本
```bash
./run_agilent_full_crawl.sh
```

**脚本会自动**：
- ✅ 检查输入文件（630个产品清单）
- ✅ 显示预计时间（45-60分钟）
- ✅ 请求确认后开始爬取
- ✅ 实时显示进度
- ✅ 自动生成报告
- ✅ 保存日志文件

#### 4. 等待完成
预计时间：45-60分钟

---

## 📦 交付文件

执行完成后，请提供以下3个文件：

### 1. 爬取结果CSV
**文件路径**: `output/agilent_full_630_results.csv`

**格式要求**：
- ✅ UTF-8编码
- ✅ 包含表头行
- ✅ 630行数据（不含表头）

**必需字段**：
- productId
- partNumber
- brand
- name（可以为空）
- description
- specifications（JSON格式）
- descriptionQuality

### 2. 爬取报告
**文件路径**: `output/agilent_full_630_report.md`

**内容包含**：
- 爬取统计（成功/失败数量）
- 数据质量分析
- 描述质量分布
- 规格完整性统计

### 3. 执行日志
**文件路径**: `logs/agilent_full_630.log`

**用途**：
- 问题排查
- 性能分析
- 质量审计

---

## 📊 质量标准

### 必达标准（不达标需重新爬取）

| 指标 | 目标 | 说明 |
|------|------|------|
| 成功率 | ≥90% | 至少567个产品成功 |
| 产品名称完整性 | 100% | 所有名称完整无截断（或为空） |
| 规格完整性 | ≥90% | 至少567个产品有≥3个规格字段 |

### 优秀标准（达到则质量优秀）

| 指标 | 目标 | 说明 |
|------|------|------|
| 成功率 | ≥95% | 至少599个产品成功 |
| 描述覆盖率 | ≥70% | 至少441个产品有描述 |
| A/B级描述占比 | ≥30% | 至少189个产品有高质量描述 |

---

## ⚠️ 注意事项

### 执行前
1. ✅ 确保Chrome浏览器已安装
2. ✅ 确保网络连接稳定
3. ✅ 确保有足够的磁盘空间（至少100MB）
4. ✅ 确保系统内存充足（至少2GB可用）

### 执行中
1. ✅ 不要关闭终端窗口
2. ✅ 不要中断脚本执行
3. ✅ 可以查看日志文件监控进度
4. ✅ 如果遇到错误，记录错误信息

### 执行后
1. ✅ 检查输出文件是否生成
2. ✅ 检查CSV文件行数（应该是631行，含表头）
3. ✅ 查看报告文件中的质量统计
4. ✅ 确认成功率达标（≥90%）

---

## 🔧 故障排除

### 问题1：Chrome WebDriver警告
**现象**：看到很多Chrome警告信息

**解决**：这是正常现象，不影响爬取结果，可以忽略

### 问题2：个别产品爬取失败
**现象**：部分产品返回404或超时

**解决**：这是正常现象，只要总成功率≥90%即可

### 问题3：爬取速度慢
**现象**：爬取时间超过预期

**解决**：
- 检查网络连接
- 检查系统资源
- 可以适当增加延迟时间（修改脚本中的DELAY参数）

### 问题4：脚本中断
**现象**：脚本执行过程中意外终止

**解决**：
- 检查错误日志
- 重新运行脚本（会自动跳过已爬取的产品）
- 如果问题严重，联系ROWELL团队

---

## 📞 联系方式

### 进度汇报
**时机**：执行完成后

**内容**：
- 总产品数：630
- 成功数量：XXX
- 失败数量：XXX
- 成功率：XX.X%
- 执行时间：XX分钟

### 问题反馈
**一般问题**：完成后汇报  
**重要问题**：立即通知  
**严重问题**：立即暂停，紧急会议

---

## ✅ 执行检查清单

执行前：
- [ ] 确认Chrome已安装
- [ ] 确认Python依赖已安装
- [ ] 确认网络连接正常
- [ ] 确认磁盘空间充足

执行中：
- [ ] 脚本正常启动
- [ ] 进度正常推进
- [ ] 无严重错误

执行后：
- [ ] 生成了CSV文件
- [ ] 生成了报告文件
- [ ] 生成了日志文件
- [ ] CSV文件行数正确（631行）
- [ ] 成功率达标（≥90%）

交付前：
- [ ] 检查了CSV文件格式
- [ ] 检查了报告内容
- [ ] 准备好日志文件
- [ ] 准备好进度汇报

---

## 🚀 开始执行

**一切准备就绪，请开始执行全量爬取任务！**

预计完成时间：45-60分钟后

我们期待收到您的高质量交付成果！💪

---

**任务发布**: 2025-11-05  
**期望完成**: 1小时内  
**优先级**: 🔴 高

**祝执行顺利！** 🎯
